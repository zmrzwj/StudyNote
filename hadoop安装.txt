/*安装Java*/
下载jdk-8u171-linux-x64.tar.gz
在/usr/local/下mkdir java
把jdk-8u171-linux-x64.tar.gz移动到/usr/local/java下
tar -zxvf jdk-8u171-linux-x64.tar.gz
修改/etc/profile,在末尾添加
export JAVA_HOME=/usr/local/java/jdk8
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$JAVA_HOME/bin:$PATH
等号与变量和路径之间不要加空格

/*hadoop用户生成*/
1.安装好jdk8
2.# useradd -m hadoop -s /bin/bash  创建新用户hadoop 
(如果出现 xxx group is exists，请使用useradd hadoop -g hadoop创建hadoop并添加给hadoop组)
(-m 自动建立用户的登入目录，-s指定用户登录后使用的shell,userdel -f zzz删除用户zzz连同目录一起删除)
3.# passwd hadoop  修改用户的密码(广思hadoop密码：hadoop)
4.# visudo 为用户增加管理员权限
找到 root ALL=(ALL) ALL 这行在这行下面增加一行内容：hadoop ALL=(ALL) ALL（当中的间隔为tab）
5.以hadoop登录，直接su hadoop不行，可以重启后以hadoop登录或logout命令
6.$ ssh localhost 测试一下 SSH 是否可用，然后退出刚才的 ssh
7.$ cd ~/.ssh/	# 若没有该目录，请先执行一次ssh localhost
8.$ ssh-keygen -t rsa	# 会有提示，都按回车就可以
9.$ cat id_rsa.pub >> authorized_keys  # 加入授权
10.$ chmod 600 ./authorized_keys    # 修改文件权限
   再用 ssh localhost 命令，无需输入密码就可以直接登陆了

/*单台机器hadoop安装*/
11.移动hadoop-2.8.4.tar.gz到/usr/local/
12.sudo tar -zvxf hadoop-2.8.4.tar.gz
13.sudo mv ./hadoop-2.6.5/ ./hadoop # 将文件夹名改为hadoop
14.sudo chown -R hadoop:hadoop ./hadoop # 修改文件权限
15.修改环境变量：
	export HADOOP_HOME=/usr/local/hadoop
	export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
	export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH

/*haddoop集群*/
15.上述操作在所有机器走一遍,关闭防火墙,切换到Hadoop用户执行以下操作
16.$ sudo vim /etc/hostname  //为了便于区分，可以修改各个节点的主机名
 添加HOSTNAME=Master.    //其它主机也对应修改为HOSTNAME=Slave1,HOSTNAME=Slave2....
17.$ sudo vim /etc/hosts //修改自己所用节点的IP映射,所有主机都需要修改
  添加: 192.168.5.114 Master
       192.168.5.115 Slave1
       192.168.5.116 Slave2
18.修改了主机名要重启：shutdown -r now

/*SSH无密码登陆节点,让 Master 节点可以无密码 SSH 登陆到各个 Slave 节点上*/
18.重新生成公匙，因为改过主机名，所以还需要删掉原有的再重新生成一次
	$ cd ~/.ssh               # 如果没有该目录，先执行一次ssh localhost
	$ rm ./id_rsa*            # 删除之前生成的公匙（如果有）
	$ ssh-keygen -t rsa       # 一直按回车就可以
	$ cat ./id_rsa.pub >> ./authorized_keys
19. Master 节点将上公匙传输到 Slave1, Slave2 节点
	$ scp ~/.ssh/id_rsa.pub hadoop@Slave1:/home/hadoop/
20.在 Slave1 节点上，将 ssh 公匙加入授权：
	$ cat ~/id_rsa.pub >> ~/.ssh/authorized_keys
	$ rm ~/id_rsa.pub    # 用完就可以删掉了
21.在 Master 节点上就可以无密码 SSH 到各个 Slave- 节点了，可在 Master 节点上执行如下命令进行检验：
	$ ssh Slave1

/*修改配置文件*/
(查看端口占用netstat -anp|grep 9200,8020,50090)
22.修改 /usr/local/hadoop/etc/hadoop 中的5个配置文件， slaves、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml 
23.文件slaves，将作为 DataNode 的主机名写入该文件，每行一个，默认为 localhost，分布式配置可以保留 localhost，也可以删掉，让 Master 节点仅作为 NameNode 使用。我们把localhost去掉，添加Slave1，Slave2.
24.修改 core-site.xml:
<configuration>
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://Master:9000</value>
        </property>
        <property>
                <name>hadoop.tmp.dir</name>
                <value>/usr/local/hadoop/tmp</value>
                <description>Abase for other temporary directories.</description>
        </property>
</configuration>
（备注：请先在 /usr/local/hadoop 目录下建立 tmp 文件夹）
25.文件 hdfs-site.xml
<configuration>
        <property>
                <name>dfs.namenode.secondary.http-address</name>
                <value>Master:50090</value>
        </property>
        <property>
                <name>dfs.replication</name>
                <value>1</value>
        </property>
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>/usr/local/hadoop/tmp/dfs/name</value>
        </property>
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>/usr/local/hadoop/tmp/dfs/data</value>
        </property>
</configuration>
（备注：请先在 /usr/local/hadoop 目录下建立 tmp/dfs/name,tmp/dfs/data 文件夹）
26.mapred-site.xml （可能需要先重命名，默认文件名为 mapred-site.xml.template），然后配置修改如下：
<configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
</configuration>
27.文件 yarn-site.xml：
<configuration>
        <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>Master</value>
        </property>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
</configuration>
28.hadoop-env.sh
export JAVA_HOME=/usr/local/java/jdk8
29.如果有其他 Slave 节点,所有节点配置都一样，拷贝过去就行。
30.格式化namenode：$hadoop namenode -format（只需要执行一次即可）(重新格式化请先删除/usr/local/hadoop/tmp)
	执行过程不报错，最后是
        /***********************************************************
	SHUTDOWN_MSG:Shutting down NameNode at Master/192.168.5.114
	************************************************************/
30.启动hadoop：$start-all.sh
 如果报错JAVA_HOME is not set and could not be found
 修改hadoop-env.sh中，JAVA_HOME
 export JAVA_HOME=${JAVA_HOME}
 改为：export JAVA_HOME=/usr/local/java/jdk8

 如果出现：namenode running as process xxxx. Stop it first.
 需要stop掉所有的hadoop服务stop-all.sh；解决：重启电脑。shutdown -r now	
30.如果没有再报错，使用jps查看节点启动情况
$jps
2000 Jps
1586 SecondaryNameNode
1739 ResourceManager
1390 NameNode
其它节点也运行$jps也会看到相应信息。
Master上的进程：NameNode，SecondaryNameNode，ResourceManager。 
Slaves上的进程：DataNode，NodeManager。
31.在master访问管理页面：http://192.168.5.114:50070/
   集群信息：http://192.168.5.114:8088
   如果访问不了，请关闭防火墙。

/*单词统计的测试wordcount*/
32.测试
$vi testWordCount 创建文件随便写点东西进去。
$ hadoop fs -mkdir /home/hadoop/input 在hdfs中创建文件夹 (如果报连接refused,就使用hadoop/bin/hadoop绝对路径允许命令)
$ hadoop fs -ls /home/hadoop/ 查看创建的文件夹
$hadoop fs -put testWordCount /home/hadoop/input //把刚创建的testWordCount文件上传到hdfs.
如果出现testwordcount._copying_ could only be replicated to 0 nodes的问题，请关闭防火墙，master和slave的都要关。systemctl stop firewalld，禁止systemctl  disable firewalld，查看firewall-cmd --state
$hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2,8,4.jar wordcount /home/hadoop/input/* /home/hadoop/output
已经编译好的WordCount的Jar在/usr/local/hadoop/share/hadoop/mapreduce目录下，其中/usr/local/hadoop是hadoop的安装目录
$hadoop fs -ls /home/hadoop/output	查看output内容
会存在两个文件xxx/_SUCCESS,xxx/part-r-00000
$hadoop fs -cat output/part-r-00000 //查看统计结果
/*
hadoop fs: 可以用于其他文件系统，不止是hdfs文件系统内,该命令的使用范围更广.
hadoop dfs :专门针对hdfs分布式文件系统.
hdfs dfs :和上面作用相同，更为推荐此命令，当使用hadoop dfs时内部会被转为hdfs dfs命令。
*/

/*
jar
运行jar文件。用户可以把他们的Map Reduce代码捆绑到jar文件中，使用这个命令执行。

用法：hadoop jar <jar> [mainClass] args...
*/


/*IDEA编写java hadoop程序*/
1.创建一个Maven项目，填写Maven的GroupId和ArtifactId（根据自己的项目随便填，点击Next）
groupId一般分为多个段，这里我只说两段，第一段为域，第二段为公司名称。我一般会将groupId设置为top.hellozwj，表示域，hellozwj是我个人姓名缩写。artifactId设置为test_hadoop，表示你这个项目的名称是test_hadoop。
2.配置依赖
apache源
在project内尾部添加
<repositories>
    <repository>
        <id>apache</id>
        <url>http://maven.apache.org</url>
    </repository>
</repositories>
阿里云仓库：
要修改maven根目录下的setting.xml文件(maven根目录在${user.home}/.m2/，修改下面的settings.xml配置本地仓库路径，没有settings这个xml文件就新建，在settings下build,execution,deployment下build tools下maven，通过user settings file可以修改settings.xml)
/*settings.xml
<settings xmlns="http://maven.apache.org/SETTINGS/1.0.0"
            xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
            xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd">
/*
<mirrors>
        <!-- 阿里云仓库 -->
        <mirror>
            <id>alimaven</id>
            <mirrorOf>central</mirrorOf>
            <name>aliyun maven</name>
            <url>http://maven.aliyun.com/nexus/content/repositories/central/</url>
        </mirror>
        <!-- 中央仓库1 -->
        <mirror>
            <id>repo1</id>
            <mirrorOf>central</mirrorOf>
            <name>Human Readable Name for this Mirror.</name>
            <url>http://repo1.maven.org/maven2/</url>
        </mirror>
    
        <!-- 中央仓库2 -->
        <mirror>
            <id>repo2</id>
            <mirrorOf>central</mirrorOf>
            <name>Human Readable Name for this Mirror.</name>
            <url>http://repo2.maven.org/maven2/</url>
        </mirror>
</mirrors> 
也可以在pom.xml添加：
<repositories><!-- 代码库 -->
        <repository>
            <id>maven-ali</id>
            <url>http://maven.aliyun.com/nexus/content/groups/public//</url>
            <releases>
                <enabled>true</enabled>
            </releases>
            <snapshots>
                <enabled>true</enabled>
                <updatePolicy>always</updatePolicy>
                <checksumPolicy>fail</checksumPolicy>
            </snapshots>
        </repository>
</repositories>
添加hadoop依赖：
基础依赖hadoop-core和hadoop-common；如果需要读写HDFS，则还需要依赖hadoop-hdfs和hadoop-client；如果需要读写HBase，则还需要依赖hbase-client。
在project内尾部添加
<dependencies>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-core</artifactId>//2.x之后不需要这个包，用hadoop-client和hadoop-hdfs两个依赖项取代
        <version>1.2.1</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>2.8.4</version>
    </dependency>
</dependencies>
//Hadoop版本为2.8.4，所以这里的maven的Hadoop版本必须对应











-------------------------------------hadoop cdh-------------------------------------------
1、cdh比原生的Apache发行版本包含了更多的补丁，用于增强稳定性，改善功能，有时候还增加功能特性 
2、cdh版本是由cloudera公司开源的，可以使用cm平台进行管理，比原生的Apache版本安装、维护更加省力 
3、但是对技术人员的要求更高，必须对原生apache版本的各个组件理解清晰 
4、在cm管理平台中，cdh的parcel包不包含某些组件，需要自己下载对应的parcel包，比如说kafka 
5、对hdfs部署过程中，对磁盘进行lvm卷轴或者是磁盘目录统一，对于多台机器，否则之后维护成本高

安装操作系统：
(vbox虚拟机选择linux2.6/3.x/4.x(64bit)不要选redhat，master内存要不少于6个G，slave不少于3个G)

1.安装jdk
修改/etc/profile,在末尾添加
export JAVA_HOME=/usr/local/java/jdk8
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$JAVA_HOME/bin:$PATH
在/usr下创建java目录，然后添加软连接
ln -s /usr/local/java  /usr/java/default

2.配置hosts
vi /etc/hosts
192.168.2.200 master
192.168.2.201 slave1
192.168.2.202 slave2

3.配置ssh免密码
cd  ~/.ssh/	# 若没有该目录，请先执行一次ssh localhost
#ssh-keygen -t rsa	# 会有提示，都按回车就可以
#scp  id_rsa.pub root@192.168.2.201:/home
#cat /home/id_rsa.pub >> ~/.ssh/authorized_keys  # 加入授权,在192.168.2.201上操作
(202执行同样的操作)
#ssh root@192.168.2.201 #测试是否可以免密码登录，在192.168.2.200上操作
（201，200上同样进行上面操作使免密码登录）

4.ntp服务配置（所有节点安装相关组件）（如果已经保证时间同步，此步骤可跳过）
yum -y install ntp ntpdate

5.在主节点安装mysql
https://cdn.mysql.com//Downloads/MySQL-5.7/mysql-5.7.24-linux-glibc2.12-x86_64.tar.gz
tar -zxvf mysql-xxxxx.glic.xxx//解压
mv mysql.xxxx.glic.xxx mysql //改名
cd mysql
Vi /etc/my.cnf
[mysql] 
# 设置mysql客户端默认字符集 
default-character-set=utf8  
socket=/var/lib/mysql/mysql.sock 
[mysqld] 
#skip-name-resolve 
#设置3306端口 
port = 3306  
socket=/var/lib/mysql/mysql.sock 
# 设置mysql的安装目录 
basedir=/usr/local/mysql 
# 设置mysql数据库的数据的存放目录 
datadir=/usr/local/mysql/data 
# 允许最大连接数 
max_connections=200 
# 服务端使用的字符集默认为8比特编码的latin1字符集 
character-set-server=utf8 
# 创建新表时将使用的默认存储引擎 
default-storage-engine=INNODB 
#lower_case_table_name=1 
max_allowed_packet=16M

创建mysql用户组：
groupadd mysql
添加mysql用户：
useradd -r -g mysql mysql

在mysql文件夹下：
mkdir data
创建文件夹：
mkdir /var/lib/mysql
chmod -R 777 /var/lib/mysql
更改mysql文件目录所有者：
chown -R mysql:mysql mysql/
./bin/mysqld --initialize --user=mysql
会有一行：
2018-06-01T02:10:16.805014Z 1 [Note] A temporary password is generated for root@localhost:   gysQ5dRwr6/j
gysQ5dRwr6/j是初始的随机密码。
设置开机启动：
1.复制启动脚本 cp ./support-files/mysql.server /etc/rc.d/init.d/mysqld
2.增加mysqld服务控制脚本执行权限 chmod +x /etc/rc.d/init.d/mysqld 
3.增加mysqld服务到系统服务  chkconfig --add mysqld
4.检查mysqld服务是否生效 chkconfig --list mysqld
出现以下则，正常了：
mysqld         	0:off	1:off	2:on	3:on	4:on	5:on	6:off
启动mysql服务：
service mysqld start
测试登陆：
mysql -u root -p
登上后，请修改密码：
set password = password("uestc@123456");
运行远程登录：
grant all privileges on *.* to root@'%' identified by 'uestc@123456' ;
grant all privileges on *.* to root@'%' identified by 'uestc@123456' WITH GRANT OPTION;
flush privileges;

创建以下数据库
create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;
create database oozie DEFAULT CHARACTER SET utf8; 
create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;
create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;

下载mysql-connector-java.jar,并保存到所有主机的/usr/share/java目录下
https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.47.tar.gz
没有/usr/share/java目录，请自行创建


6.官网下载Cloudera Manager和CDH
Cloudera Manager下载地址：
http://archive.cloudera.com/cm5/cm/5/
cloudera-manager-centos7-cm5.15.1_x86_64.tar.gz
CDH安装包:
http://archive.cloudera.com/cdh5/parcels/latest/
CDH-5.11.0-1.cdh5.15.1.p0.34-el6.parcel
CDH-5.11.0-1.cdh5.15.1.p0.34-el6.parcel.sha1
manifest.json
(manager与CDH版本要一致，vbox虚拟机选择linux2.6/3.x/4.x(64bit)不要选redhat)

解压cloudera-manager-centos7-cm5.15.1_x86_64.tar.gz解压后会出现cloudera,cm-5.15.1两个目录
mv cloudera /opt
mv cm-5.15.1 /opt
复制mysql-connector-java.jar到目录/opt/cm-5.15.1/share/cmf/lib/

7.创建用户cloudera-scm（所有节点操作）
由于Cloudera Manager和Managed Services默认使用cloudera-scm，所以需要创建此用户
useradd --system --no-create-home --shell=/bin/false --comment "Cloudera SCM User" cloudera-scm

8.agent配置
修改/opt/cm-5.15.1/etc/cloudera-scm-agent/config.ini中的server_host为主节点的主机名。
# Hostname of the CM server.
server_host=master

9.在主节点初始化CM5数据库
/opt/cm-5.15.1/share/cmf/schema/scm_prepare_database.sh mysql cm -h master -uroot -puestc@123456 --scm-host master scm scm scm

mysql后到cm随便取名字，mysql会创建一个cm的数据库
-h后是mysql的主机名
--scm-host 后是SCM server的主机名（主节点）
(注意：-uroot，u和root不空格 -puestc@123456，p和uestc@123456不空格)

10.准备Parcels，用以安装CDH5,(只主节点)
将CHD5相关的Parcel包放到主节点的/opt/cloudera/parcel-repo/目录中
cp CDH-5.15.1-1.cdh5.15.1.p0.4-el5.parcel /opt/cloudera/parcel-repo/
cp CDH-5.15.1-1.cdh5.15.1.p0.4-el5.parcel.sha1 /opt/cloudera/parcel-repo/
cp manifest.json /opt/cloudera/parcel-repo/
相关的文件如下：
CDH-5.15.1-1.cdh5.11.0.p0.34-el6.parcel
CDH-5.15.1-1.cdh5.11.0.p0.34-el6.parcel.sha1
manifest.json
最后将xxx.parcel.sha1，重命名为xxx.parcel.sha
mv /opt/cloudera/parcel-repo/CDH-5.15.1-1.cdh5.15.1.p0.4-el5.parcel.sha1 /opt/cloudera/parcel-repo/CDH-5.15.1-1.cdh5.15.1.p0.4-el5.parcel.sha

11.启动主节点的CM  Server和所有节点的Agent
主节点：通过/opt/cm-5.15.1/etc/init.d/cloudera-scm-server start启动服务端。
所有节点（包括主节点）：通过/opt/cm-5.15.1/etc/init.d/cloudera-scm-agent start启动Agent服务
停止可以用/cloudera-scm-server stop
(问题：
cloudera-scm-agent start Starting 
cloudera-scm-agent: [FAILED]
解决：查看日志
cd /opt/cm-5.7.0/log/cloudera-scm-agent/ 
cat cloudera-scm-agent.out
如果提示python有问题，请使用cloudera-manager-centos7，而不是el版的
)

12.访问http://192.168.2.200:7180/cmf/login
用户名：admin，密码：admin
注意选免费版，回退很难
删除cm数据库，重新初始化。
 问题：Parcel不可用于操作系统分配RHEL7
 解决：manager与CDH版本要一致，vbox虚拟机选择linux2.6/3.x/4.x(64bit)不要选redhat
 问题：出现主机运行状态不良情况
 解决：rm -f /opt/cm-5.15.1/lib/cloudera-scm-agent/cm_guid (每个节点)
     /opt/cm-5.15.1/etc/init.d/cloudera-scm-agent restart
 问题：Cloudera 建议将 /proc/sys/vm/swappiness 设置为最大值 10
      echo 'vm.swappiness=10'>> /etc/sysctl.conf

1)选择安装服务
我选择的‘所有服务’
2）集群设置角色分配，没特殊要求的话，选默认即可，点继续
3）数据库设置
请使用安装mysql时创建的数据库,点击连接测试，当全部都显示Successful，点击继续
4)群集设置,审核更改,选择默认，点击继续

find / -type f -name "*cc.sh"
定位到编辑 vim /opt/cm-5.15.1/lib64/cmf/service/client/deploy-cc.sh
直接在上面加上
JAVA_HOME=/usr/local/java
export JAVA_HOME=/usr/local/java
所有节点都这样设置一下

安装失败后：
删除mysql下的cm数据库，重新初始化，重启server, agent

问题：安装CDH找不到java_home的错误
jdk的时候,应该要安装到/usr/java目录
ln -s /usr/local/java  /usr/java/default

安装好后cloudera-scm-server启动很慢，大概6分钟

问题：hue webUI无法访问，请在cloudera manager主页点击hue，在hue的配置下点击‘端口和地址’，勾选将 Hue 服务器绑定到通配符地址，重启服务。
（注：hdfs的50070，spark,yarn等都是因为没有把端口是和外网地址绑定，所以把端口的地址都绑定到0.0.0.0，外网才能访问）
问题：hdfs副本不足的块
dfs. replication属性默认是3，也就是说副本数---块的备份数默认为3份。但是我们这里集群只有两个DataNode.
点击HDFS-配置,在搜索框搜索dfs. replication，设置为2后保存更改

13.介绍
JobHistoryServer
         历史服务器，管理者可以通过历史服务器查看已经运行完成的Mapreduce作业记录，比如用了多少个Map、多少个Reduce、作业提交时间、作业启动时间、作业完成时间等信息。
Impala
	Impala是Cloudera公司主导开发的新型查询系统，它提供SQL语义，能查询存储在Hadoop的HDFS和HBase中的PB级大数据。已有的Hive系统虽然也提供了SQL语义，但由于Hive底层执行使用的是MapReduce引擎，仍然是一个批处理过程，难以满足查询的交互性。相比之下，Impala的最大特点也是最大卖点就是它的快速。
HUE
	Hue是cdh专门的一套web管理器，它包括3个部分hue ui，hue server，hue db。hue提供所有的cdh组件的shell界面的接口。你可以在hue编写mr，查看修改hdfs的文件，管理Hive的元数据，运行Sqoop，编写Oozie工作流等大量工作。



14.Hadood中的NameNode和ResourcManager是集群中的重要角色，如果这两个角色出现问题将导致整个集群无法使用。所以保证这两个角色的高可用是保证整个hadoop分布式系统高可用的关键。





---------------------------------------HDP(Hortonworks Data Platform)---------------------------------
1.安装jdk
修改/etc/profile,在末尾添加
export JAVA_HOME=/usr/local/java/jdk8
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$JAVA_HOME/bin:$PATH
在/usr下创建java目录，然后添加软连接
ln -s /usr/local/java  /usr/java/default

2.配置hosts
vi /etc/hosts
192.168.2.200 master
192.168.2.201 slave1
192.168.2.202 slave2

3.配置ssh免密码
cd  ~/.ssh/	# 若没有该目录，请先执行一次ssh localhost
#ssh-keygen -t rsa	# 会有提示，都按回车就可以
#scp  id_rsa.pub root@192.168.2.201:/home
#cat /home/id_rsa.pub >> ~/.ssh/authorized_keys  # 加入授权,在192.168.2.201上操作
(202执行同样的操作，master自己也要)
#ssh root@192.168.2.201 #测试是否可以免密码登录，在192.168.2.200上操作
（201，200上同样进行上面操作使免密码登录）


3.安装ambari server
下载ambari repo文件 ，并拷贝到/etc/yum.repos.d中
wget -nv http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.6.2.2/ambari.repo -O /etc/yum.repos.d/ambari.repo
-nv, –non-verbose 关掉冗长模式，但不是安静模式
-O –output-document=FILE 把文档写到FILE文件中
yum install ambari-server
配置：
ambari-server setup
WARNING: SELinux is set to 'permissive' mode and temporarily disabled.
OK to continue [y/n] (y)? y
Customize user account for ambari-server daemon [y/n] (n)? y
Enter user account for ambari-server daemon (root):root
Adjusting ambari-server permissions and ownership...
Checking firewall status...
Checking JDK...
Do you want to change Oracle JDK [y/n] (n)? y
[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8
[2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7
[3] Custom JDK
==============================================================================
Enter choice (1): 1 （如果选3，请先安装Java，后面会提示输入java_home）
Configuring database...
==============================================================================
Choose one of the following options:
[1] - PostgreSQL (Embedded)
[2] - Oracle
[3] - MySQL / MariaDB
[4] - PostgreSQL
[5] - Microsoft SQL Server (Tech Preview)
[6] - SQL Anywhere
[7] - BDB
==============================================================================
Enter choice (1): 1 
（
如果要用MySQL，请先安装
#进入数据库创建ambari库和导入数据
mysql>create database ambari DEFAULT CHARSET utf8;
mysql>use ambari
mysql>source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql
）
#启动服务
ambari-server start
#停止服务
ambari-server stop
#重置服务
ambari-server reset



4、安装httpd
# yum -y install httpd
# service httpd restart
# chkconfig httpd on
# cd /var/www/html/
# mkdir ambari（下载好的离线包解压在这里）
# cd ambari
# mkdir HDP
# mkdir HDP-UTILS



5、安装mysql5.7
wget http://repo.mysql.com/mysql57-community-release-el7-8.noarch.rpm
rpm -ivh mysql57-community-release-el7-8.noarch.rpm
yum install mysql-server
systemctl start mysqld
vim /var/log/mysqld.log(查看初始root密码)
找到 A temporary password is generated for root@localhost: f-BcwmSCq0Ot
mysql -uroot -p
set password = password("Sccddw123456!");(5.7.6以前)
 ALTER USER USER() IDENTIFIED BY 'Sccddw123456!';(5.7.6以后)
 (问题：Your password does not satisfy the current policy requirements？ 因为密码强度不够
 SHOW VARIABLES LIKE 'validate_password%'; 查看 mysql 初始的密码策略
 set global validate_password_policy=LOW;  将密码强度降低
 )
  grant all privileges on *.* to 'root'@'%' identified by 'Sccddw123456!';
 为hive，oozie创建数据库：
CREATE DATABASE hive;  
use hive;  
CREATE USER 'hive'@'%' IDENTIFIED BY 'hive';  
GRANT ALL PRIVILEGES ON *.* TO 'hive'@'%';  
CREATE USER 'hive'@'localhost' IDENTIFIED BY 'hive';  
GRANT ALL PRIVILEGES ON *.* TO 'hive'@'localhost';  
CREATE USER 'hive'@'master' IDENTIFIED BY 'hive';  
GRANT ALL PRIVILEGES ON *.* TO 'hive'@'master';  
FLUSH PRIVILEGES;  
CREATE DATABASE oozie;  
use oozie;  
CREATE USER 'oozie'@'%' IDENTIFIED BY 'oozie';  
GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'%';  
CREATE USER 'oozie'@'localhost' IDENTIFIED BY 'oozie';  
GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'localhost';  
CREATE USER 'oozie'@'master' IDENTIFIED BY 'oozie';  
GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'master'; 
FLUSH PRIVILEGES;
 
下载mysql-connector-java-5.1.48.jar
https://mvnrepository.com/artifact/mysql/mysql-connector-java
mkdir /usr/share/java
cp mysql-connector-java-5.1.40.jar /usr/share/java/mysql-connector-java.jar
cp /usr/share/java/mysql-connector-java.jar /var/lib/ambari-server/resources/mysql-jdbc-driver.jar
vi  /etc/ambari-server/conf/ambari.properties
添加server.jdbc.driver.path=/usr/share/java/mysql-connector-java.jar
ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar




6.下载离线包
(版本请访问：https://supportmatrix.hortonworks.com/)
ambari:(ambari2.6.2.2为2.6最高版本，ambari2.7仅支持hdp3)
http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.6.2.2/ambari-2.6.2.2-centos7.tar.gz
HDP:(hdp2.6.5为2.6最高版本，hdp3.0不支持flume)
http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.5.0/HDP-2.6.5.0-centos7-rpm.tar.gz
HDP-UTILS:
http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7/HDP-UTILS-1.1.0.21-centos7.tar.gz
HDP-GPL:
http://public-repo-1.hortonworks.com/HDP-GPL/centos7/2.x/updates/2.6.5.0/HDP-GPL-2.6.5.0-centos7-gpl.tar.gz
tar -zxvf HDP-2.6.3.0-centos7-rpm.tar.gz -C /var/www/html/ambari/HDP
tar -zxvf HDP-UTILS-1.1.0.21-centos7.tar.gz -C /var/www/html/ambari/HDP-UTILS
tar -zxvf ambari-2.6.1.5-centos7.tar.gz -C /var/www/html/ambari/ambari-2.6.1.5
设置离线源
vi /etc/yum.repos.d/ambari.repo
#VERSION_NUMBER=2.6.1.5-3
[ambari-2.6.1.5]
name=ambari Version - ambari-2.6.1.5
baseurl=http://192.168.11.211/ambari/ambari-2.6.1.5/ambari/centos7/2.6.1.5-3/
gpgcheck=1
gpgkey=http://192.168.11.211/ambari/ambari-2.6.1.5/ambari/centos7/2.6.1.5-3/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
enabled=1
priority=1

vi /etc/yum.repos.d/hdp.repo
#VERSION_NUMBER=2.6.3.0-235
[HDP-2.6.3.0]
name=HDP Version - HDP-2.6.3.0
baseurl=http://192.168.11.211/ambari/HDP/centos7/2.6.3.0-235/
gpgcheck=1
gpgkey=http://192.168.11.211/ambari/HDP/centos7/2.6.3.0-235/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
enabled=1
priority=1

[HDP-UTILS-1.1.0.21]
name=HDP-UTILS Version - HDP-UTILS-1.1.0.21
baseurl=http://192.168.11.211/ambari/HDP-UTILS
gpgcheck=1
gpgkey=http://192.168.11.211/ambari/HDP-UTILS/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
enabled=1
priority=1

在线源：
先备份：
mv /etc/yum.repos.d/CentOS-Base.repo  /etc/yum.repos.d/CentOS-Base.repo.bak
阿里源：
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/.help/CentOS6-Base-163.repo





7、安装平台
浏览器访问：http://master:8080 默认用户名密码admin/admin
1).点击‘Launch install wizard’
2).配置集群的名字为sccddw
3).选择版本并修改为本地源地址
选择redhat7:
	HDP-2.6: http://192.168.11.211/ambari/HDP/centos7/2.6.3.0-235/
	HDP-UTILS-1.1.0.21: http://192.168.11.211/ambari/HDP-UTILS/
4).安装配置
	Target Hosts：
		master
		slave1
		slave2
	选择要安装的服务器和上传上面教程所拷贝出来的秘钥文件id_rsa
可能错误：
	1）SSLError: Failed to connect. Please check openssl library versions
	vim /etc/python/cert-verification.cfg
	[https]
		verify=platform_default
		修改为verify=disable，就可以正常注册。
	vi /etc/ambari-agent/conf/ambari-agent.ini
	[security]
		force_https_protocol=PROTOCOL_TLSv1_2
	(升级openssl:(不一定需要)
		yum upgrade openssl)

	2）提示namenode不能以home开头
	vi /usr/lib/ambari-server/web/javascripts/app.js
	isAllowedDir: function(value) {
		var dirs = value.replace(/,/g,' ').trim().split(new RegExp("\\s+", "g"));
		for(var i = 0; i < dirs.length; i++){
			if(dirs[i].startsWith('/home') || dirs[i].startsWith('/homes')) {
				return false;
			}
		}
		return true;
	}
	把false改为true
	(每个节点都要做)
	
Customize Services:
	除了hive, oozie需要填写上面mysql相应创建的密码。其它都是填要设置的初始密码随便填但要做好记录。
设置hive和oozie时点击‘Test Connection’测试是否可以连接成功。出错时看Database URL是否设置成功。

(注意事项：
hive：Tez Container Size最好不超过5120M
Ambari Metrics: hbase_master_heapsize 尽量大些
Ambari Metrics	hbase_master_xmn_size 尽量大些
Ambari Metrics	metrics_collector_heapsize 尽量大些
Hive：hive.auto.convert.join.noconditionaltask.size 不超过5120M
)
安装出错请查看日志：
agent端：
cd /var/log/ambari-agent/
tail -f ambari-agent.log
server端：
cd /var/log/ambari-server/
tail -f ambari-server.log




8.安装完成后启动
出现问题后一一排查
strom问题：
日志：/var/log/storm/
（
如果报missing:xxx/jmxetric-xxx.jar
修改nimbus.childopts配置：去掉jmx相关的只保留-Xmx1024m _JAAS_PLACEHOLDER
supervisor.childopts同样也要去掉
）
Faclon:
日志：/var/log/faclon/faclon.application.log
Faclou web ui 无法启动：
HDP 2.5或更高版本，则必须手动安装Falcon所需的Berkeley DB文件(在ambari server上)
下载je.jar: http://search.maven.org/remotecontent?filepath=com/sleepycat/je/5.0.73/je-5.0.73.jar
mv remotecontent?filepath=com%2Fsleepycat%2Fje%2F5.0.73%2Fje-5.0.73.jar /usr/share/je-5.0.73.jar
ambari-server setup --jdbc-db=bdb --jdbc-driver=/usr/share/je-5.0.73.jar
ambari-server restart


9.卸载ambari
停止ambari:
ambari-agent stop
ambari-server stop
卸载ambari：
yum remove ambari-agent  
yum remove ambari-server
删除服务：
yum list installed | grep ambari  
yum list installed | grep hdp  
yum remove -y hive*   
yum remove -y hdfs*   
yum remove -y yarn*   
yum remove -y mapreduce2*   
yum remove -y tez*   
yum remove -y hbase*   
yum remove -y pig*   
yum remove -y sqoop*   
yum remove -y oozie*   
yum remove -y zookeeper*   
yum remove -y falcon*   
yum remove -y storm*   
yum remove -y flume*   
yum remove -y accumulo*   
yum remove -y Ambari Infra*   
yum remove -y Ambari Metrics*   
yum remove -y Atlas*   
yum remove -y Kafka*   
yum remove -y Knox*   
yum remove -y Log Search*   
yum remove -y Ranger*   
yum remove -y Ranger KMS*   
yum remove -y SmartSense*   
yum remove -y Zeppelin Notebook*   
yum remove -y Druid*   
yum remove -y Mahout*   
yum remove -y Slider* 
yum remove -y hadoop*
yum remove -y spark*
yum remove -y strom*
删除用户
userdel nagios   
userdel hive   
userdel ambari-qa   
userdel hbase   
userdel oozie   
userdel hcat   
userdel mapred   
userdel hdfs   
userdel rrdcached   
userdel zookeeper   
userdel mysql   
userdel sqoop  
userdel puppet  
userdel yarn  
userdel tez  
userdel hadoop  
userdel knox  
userdel storm  
userdel falcon  
userdel flume  
userdel nagios  
userdel admin  
userdel postgres  
userdel  hdfs  
userdel  zookeeper  
userdel hbase  
userdel ams       
userdel spark     
userdel kafka     
userdel ranger    
userdel kms       
userdel zookeeper     
userdel ambari-qa     
userdel hdfs      
userdel yarn      
userdel mapred 
删除文件
rm -rf /hadoop/*  
rm -rf /etc/ambari*  
rm -rf /etc/zookeeper/  
rm -rf /etc/ranger*  
rm -rf /etc/rc.d/init.d/ranger*  
rm -rf /etc/hadoop/  
rm -rf /etc/hadoop/  
rm -rf /etc/hbase/  
rm -rf /etc/hive  
rm -rf /usr/hdp/  
rm -rf /etc/zookeeper/  
rm -rf /tmp/ambari-qa  
rm -rf /tmp/sqoop-ambari-qa/  
rm -rf /kafka-logs/  
rm -rf /var/lib/ambari*  
rm -rf /var/lib/hive2*  
rm -rf /var/run/kafka/  
rm -rf /etc/flume     
rm -rf /etc/hive-hcatalog     
rm -rf /etc/hive-webhcat      
rm -rf /etc/phoenix       
rm -rf /etc/ambari-metrics-collector      
rm -rf /etc/ambari-metrics-monitor    
rm -rf /tmp/hdfs      
rm -rf /tmp/hcat      
rm -rf /etc/kafka  
rm -rf /etc/oozie     
rm -rf /etc/storm     
rm -rf /etc/tez       
rm -rf /etc/falcon    
rm -rf /etc/slider    
rm -rf /etc/pig       
rm -rf /etc/ranger    
rm -rf /var/log/hadoop    
rm -rf /var/log/hbase     
rm -rf /var/log/hive      
rm -rf /var/log/oozie     
rm -rf /var/log/sqoop     
rm -rf /var/log/zookeeper     
rm -rf /var/log/flume     
rm -rf /var/log/storm     
rm -rf /var/log/hive-hcatalog     
rm -rf /var/log/falcon    
rm -rf /var/log/webhcat       
rm -rf /var/log/hadoop-hdfs       
rm -rf /var/log/hadoop-yarn       
rm -rf /var/log/hadoop-mapreduce      
rm -rf /var/log/spark     
rm -rf /var/log/ranger    
rm -rf /usr/lib/flume     
rm -rf /usr/lib/storm     
rm -rf /var/lib/hive      
rm -rf /var/lib/oozie     
rm -rf /var/lib/zookeeper     
rm -rf /var/lib/flume     
rm -rf /var/lib/hadoop-hdfs       
rm -rf /var/lib/slider    
rm -rf /var/lib/ranger    
rm -rf /var/tmp/oozie     
rm -rf /var/tmp/sqoop     
rm -rf /tmp/hive      
rm -rf /tmp/hadoop-hdfs       
rm -rf /etc/sqoop     
rm -rf /var/log/ambari-metrics-collector      
rm -rf /var/log/ambari-metrics-monitor    
rm -rf /usr/lib/ambari-metrics-collector      
rm -rf /var/lib/hadoop-yarn       
rm -rf /var/lib/hadoop-mapreduce      
rm -rf /var/lib/ambari-metrics-collector      
rm -rf /var/log/kafka     
rm -rf /tmp/hadoop-yarn       
rm -rf /hadoop/zookeeper      
rm -rf /hadoop/hdfs       
rm -rf /kafka-logs    
rm -rf /etc/storm-slider-client

yum remove -y hadoop_2* hdp-select* ranger_2* zookeeper* bigtop* atlas-metadata* ambari* spark* slide* strom* hive*
rpm –qa | grep Ambari版本
rm -rf /var/lib/pgsql/
yum remove -y postgresql
把 /usr/hdp也删除






10.服务介绍
Ambari  Hadoop生态系统管理监控。
PostgreSQL  Ambari内置数据库。
Hdfs 分布式文件系统。
Yarn 资源调度管理。
MapReduce是一个分布式运算程序的编程框架,是hadoop数据分析的核心。
Tez  Tez是在YARN之上编写的下一代Hadoop查询处理框架。
Hive 数据仓库系统，用于临时查询和分析大型数据集以及表和存储管理服务。
HBase 分布式列数据库。
Pig  Pig是MapReduce的一个抽象，它是一个工具/平台，用于分析较大的数据集，并将它们表示为数据流。
Sqoop  用于在Hadoop(Hive)与传统的数据库(mysql、postgresql...)间进行数据的传递。
Oozie  用于Hadoop平台的开源的工作流调度引擎，是用来管理Hadoop作业。
ZooKeeper  开放源码的分布式应用程序协调服务，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。
Falcon  一个开源的hadoop数据生命周期管理框架, 它提供了数据源的管理服务
Storm  一个分布式实时大数据处理系统。
Flume  一种分布式，可靠且可用的服务，用于有效地收集，聚合和移动大量日志数据。
Accumulo  一个可靠的、可伸缩的、高性能的排序分布式的 Key-Value 存储解决方案，基于单元访问控制以及可定制的服务器端处理。
Ambari Infra  为安装栈上组件提供公共索引服务。
Ambari Metrics  为系统管理员提供了集群性能的监察功能。
Atlas  为Hadoop集群提供了包括 数据分类、集中策略引擎、数据血缘、安全和生命周期管理在内的元数据治理核心能力。
Kafka  开源消息系统，为处理实时数据提供一个统一、高通量、低等待的平台。
Knox   是一个 REST API 网关，用于提供对数据的安全访问和处理 Hadoop 集群的资源。
SmartSense  用于收集群集诊断数据，用于协助支持案例故障排除和SmartSense分析。
Spark2  大规模数据处理而设计的快速通用的计算引擎。
Zeppelin Notebook  一个让交互式数据分析变得可行的基于网页的notebook。Zeppelin提供了数据可视化的框架。
Mahout    一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。
Apache Slider    将用户的已有服务或者应用直接部署到YANR上。





(
Accumulo root pwd: accumulo
Instance Secret: accumulo
Superset: superset
)

sccddw密码：
PostgreSQL:
Database admin user (postgres): 
Database name (ambari): 
Postgres schema (ambari): 
Username (ambari): 
Enter Database Password (bigdata): 
ambari webUI：
http:/192.168.11.211:8080 默认用户名密码admin/admin
Accumulo root password：accumulo123###
Instance Secret: accumulo123###
smartSense:
Password for user 'admin'： smartSense
Knox Master Secret: knox123###
mysql:
user: root
password: Sccddw123456!
hive:
database: hive
user: hive
password：hive
oozie:
database: oozie
user: oozie
password：oozie
Grafana Admin Password: grafana











